# DACON - 신용카드 연체 정도 예측

> 화이팅!
>
> [데이터 설명_데이콘](https://www.dacon.io/competitions/official/235713/talkboard/402821/)

## 데이터 전처리

[데이터 전처리의 모든 것](http://www.dodomira.com/2016/10/20/how_to_eda/) : 해당 링크처럼 전처리 단계 진행하기. 

----------------------------------------------------------------------------

* 범주형 변수의 처리

  * 링크:  [Machine Learning\][머신러닝] 데이터 전처리(범주형/연속형) (tistory.com)](https://ysyblog.tistory.com/71)

  1) 레이블 인코딩 

  * 숫자의 차이가 모델에 영향을 주지 않는 트리 계열 모델 (의사결정나무, 랜덤포레스트)에 적용
    * LGBM도 트리 계열 모델 , [lgith gbm의 장점 질문 - 인프런 | 질문 & 답변 (inflearn.com)](https://www.inflearn.com/questions/40387)
    * 다만, 다른 모델 사용 시 영향을 줄 수 있기에 일단 간편하게 원핫인코딩 하였음. 
  * 숫자의 차이가 모델에 영향을 미치는 선형 계열 모델(로지스틱회귀, SVM, 신경망)에는 사용 x

  2) 원핫 인코딩

  * 범주형 모두 원핫 인코딩 (베이스 코드)

-------------

* 결측치 처리 (O)

* 이상치 처리 

  * 정규화 전에 이상치 제거하면 더 좋음.

* 정규화

  * 링크: [정규화(Normalization)의 목적과 방법들 (tistory.com)](https://mole-starseeker.tistory.com/31)
  * 데이터 정규화는 어떤 feature가 다른 feature를 지배하지 못하도록 만드는 과정이고, 결국 모델이 더 잘 알아들을 수 있도록 데이터를 정제하는 과정

  * 링크: [어떤 스케일러를 쓸 것인가?]([[Python\] 어떤 스케일러를 쓸 것인가? (mkjjo.github.io)](https://mkjjo.github.io/python/2019/01/10/scaler.html))
    * 대부분 우리가 다루는 데이터 중 **금액처럼 큰 수치 데이터에 로그를 취하게 되는 이유** 이기도 하다. 보통 이런 데이터는 선별적으로 로그를 취한 후 모델링 전 전반적으로 스케일링을 적용한다.(in my opinion)
    * 결론적으로 모든 스케일러 처리 전에는 아웃라이어 제거가 선행되어야 한다. 또한 데이터의 분포 특징에 따라 적절한 스케일러를 적용해주는 것이 좋다.
    * 유의해야할 점은, 스케일링시 Feature별로 크기를 유사하게 만드는 것은 중요하지만, 그렇다고 모든 Feature의 분포를 동일하게 만들 필요는 없다.
    * 특성에 따라 어떤 항목은 원본데이터의 분포를 유지하는 것이 유의할 수 있다. 예로 데이터가 거의 한 곳에 집중되어 있는 Feature를 표준화시켜 분포를 같게 만들었을때 작은 단위의 변화가 큰 차이를 나타내는 것으로 반영될 수 있기 때문이다.

* 중복데이터 처리

---

### 

* 회귀분석
* scaler 어떤 것을 적용할 지
* 예측 모델을 어떤 알고리즘을 사용할 지

* One Hot Encoding , LabelEncoding



## 개선 방안 

* 직업과 소득의 상관관계를 찾아서 비슷한 소득인 사람의 직업으로 NULL값 채우기

* 아이가 17명이다, 부모님이 (-)다. 

  

## 요인

* DAYS_BIRTH 는 양수/연차로 바꾸고
* DAYS_EMPLOYEE 는 양수 연차, 365243 --> 0년차
* begin_month 양수처리만
* child num 